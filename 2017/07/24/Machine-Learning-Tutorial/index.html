<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    <title>机器学习（Machine Learning）简单学 | 指尖の岁月 | 世间点滴，莫忘于心</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="Machine Learning,Deep Learning,Reinforcement Learning">
    <meta name="description" content="本人对于机器学习这项技术具有强烈的兴趣，但是网络上的文献鱼龙混杂，很难找到真正适合入门级别的新手观看。前一阵子无意间在网络上看到了莫烦Python的教学视频，发现其中的内容丰富有趣并且具有很好的阶层学习框架。于是总结了一些精髓并加入了自己从事机器学习研究所工作的一些见解，总结了一些精华的部分以供大家快速入门和学习。 什么是机器学习机器学习（Machine Learning）是由一帮计算机科学家们希">
<meta name="keywords" content="Machine Learning,Deep Learning,Reinforcement Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习（Machine Learning）简单学">
<meta property="og:url" content="http://yoursite.com/2017/07/24/Machine-Learning-Tutorial/index.html">
<meta property="og:site_name" content="指尖の岁月">
<meta property="og:description" content="本人对于机器学习这项技术具有强烈的兴趣，但是网络上的文献鱼龙混杂，很难找到真正适合入门级别的新手观看。前一阵子无意间在网络上看到了莫烦Python的教学视频，发现其中的内容丰富有趣并且具有很好的阶层学习框架。于是总结了一些精髓并加入了自己从事机器学习研究所工作的一些见解，总结了一些精华的部分以供大家快速入门和学习。 什么是机器学习机器学习（Machine Learning）是由一帮计算机科学家们希">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://i.imgur.com/PSM7e7e.png">
<meta property="og:image" content="https://i.imgur.com/hq1q9Z5.png">
<meta property="og:image" content="https://i.imgur.com/8Nj3E0r.png">
<meta property="og:image" content="https://i.imgur.com/ltVHIxt.png">
<meta property="og:image" content="http://chart.googleapis.com/chart?cht=tx&chl=\Large {A \over A + B}">
<meta property="og:image" content="https://i.imgur.com/9vwwsJt.png">
<meta property="og:image" content="http://chart.googleapis.com/chart?cht=tx&chl=\Large Entropy(S) = {-(9 / 14) * log2(9 / 14) - (5 / 14) * log2(5 / 14)}">
<meta property="og:image" content="http://chart.googleapis.com/chart?cht=tx&chl=\Large Entropy(Weak) = {-(6 / 8) * log2(6 / 8) - (2 / 8) * log2(2 / 8)}">
<meta property="og:image" content="http://chart.googleapis.com/chart?cht=tx&chl=\Large Entropy(Strong) = {-(3 / 6) * log2(3 / 6) - (3 / 6) * log2(3 / 6)}">
<meta property="og:image" content="http://chart.googleapis.com/chart?cht=tx&chl=\Large Gain(Wind) = {Entropy(S) -(8 / 14) * Entropy(Weak) - (6 / 14) * Entropy(Strong)}">
<meta property="og:image" content="https://i.imgur.com/Ea3Jb95.png">
<meta property="og:image" content="https://i.imgur.com/NGsSXtM.png">
<meta property="og:image" content="https://i.imgur.com/5seIoZJ.png">
<meta property="og:image" content="http://chart.googleapis.com/chart?cht=tx&chl=\Large P(c | x) = {P(x | c) P(c) \over P(x)}">
<meta property="og:image" content="https://i.imgur.com/gBuFCBd.png">
<meta property="og:image" content="https://i.imgur.com/eSwuOJV.png">
<meta property="og:image" content="https://i.imgur.com/qZPw7xC.png">
<meta property="og:image" content="http://chart.googleapis.com/chart?cht=tx&chl=\Large Posterior(male) = {P(male) P(height | male) P(weight | male) P(footsize | male) \over evidence}">
<meta property="og:image" content="http://chart.googleapis.com/chart?cht=tx&chl=\Large Posterior(female) = {P(female) P(height | female) P(weight | female) P(footsize | female) \over evidence}">
<meta property="og:image" content="http://chart.googleapis.com/chart?cht=tx&chl=\Large Evidence = {(Posterior(female) + Posterior(male)) * evidence}">
<meta property="og:image" content="http://chart.googleapis.com/chart?cht=tx&chl=\Large P(height | male) = {1 \over \sqrt{2\pi\sigma^2}}exp({-(6 - \mu^2) \over 2\sigma^2})">
<meta property="og:image" content="http://chart.googleapis.com/chart?cht=tx&chl=\Large P(weight | male) = ...">
<meta property="og:image" content="http://chart.googleapis.com/chart?cht=tx&chl=\Large Posterior Numerator(male) = {6.1984e^{-09}}">
<meta property="og:image" content="http://chart.googleapis.com/chart?cht=tx&chl=\Large Posterior Numerator(female) = {5.3778e^{-04}}">
<meta property="og:image" content="https://i.imgur.com/7sGrxz0.png">
<meta property="og:image" content="http://chart.googleapis.com/chart?cht=tx&chl=\Large d(x, y) := {\sqrt{(X1 - Y1)^2 + (X2 - Y2)^2 + ... + (Xn - Yn)^2}}">
<meta property="og:image" content="http://chart.googleapis.com/chart?cht=tx&chl=\Large {|X1 - Y1| + |X2 - Y2| + ... + |Xn - Yn|}">
<meta property="og:image" content="http://chart.googleapis.com/chart?cht=tx&chl=\Large {(|X1 - Y1|^p + |X2 - Y2|^p + ... + |Xn - Yn|^p)^{1 \over p}}">
<meta property="og:image" content="https://i.imgur.com/WQlIGo4.png">
<meta property="og:image" content="https://i.imgur.com/xViexYM.png">
<meta property="og:image" content="https://i.imgur.com/eOKOw6J.png">
<meta property="og:image" content="https://i.imgur.com/umtL8L5.png">
<meta property="og:image" content="https://i.imgur.com/LT2BXvM.png">
<meta property="og:image" content="https://i.imgur.com/vYgqqHJ.png">
<meta property="og:image" content="https://i.imgur.com/UHIK2YN.png">
<meta property="og:image" content="https://i.imgur.com/so90M0c.png">
<meta property="og:image" content="https://i.imgur.com/Y65bdvJ.png">
<meta property="og:image" content="https://i.imgur.com/TZ1jOeO.png">
<meta property="og:image" content="https://i.imgur.com/uOav6gV.png">
<meta property="og:image" content="https://i.imgur.com/fScPOCU.png">
<meta property="og:image" content="https://i.imgur.com/VmMQcSI.png">
<meta property="og:image" content="https://i.imgur.com/twaqQCR.png">
<meta property="og:image" content="https://i.imgur.com/ikChVno.png">
<meta property="og:image" content="https://i.imgur.com/zugYmYR.png">
<meta property="og:image" content="https://i.imgur.com/T3B3QjS.png">
<meta property="og:image" content="https://i.imgur.com/afEYtWN.png">
<meta property="og:image" content="https://i.imgur.com/AUqu2Ss.png">
<meta property="og:image" content="https://i.imgur.com/D86UcOA.png">
<meta property="og:image" content="https://i.imgur.com/MW1BRcu.png">
<meta property="og:image" content="https://i.imgur.com/iQKtiQP.png">
<meta property="og:image" content="https://i.imgur.com/aZqWHqb.png">
<meta property="og:image" content="https://i.imgur.com/Noc6AFL.png">
<meta property="og:image" content="https://i.imgur.com/2zP4G0h.png">
<meta property="og:image" content="https://i.imgur.com/LJSO3JG.png">
<meta property="og:image" content="https://i.imgur.com/SfOfLbo.png">
<meta property="og:image" content="https://i.imgur.com/BidenDF.png">
<meta property="og:image" content="https://i.imgur.com/lOfWUK4.png">
<meta property="og:image" content="https://i.imgur.com/tU6ZZEW.png">
<meta property="og:image" content="https://i.imgur.com/a3YNuV0.png">
<meta property="og:image" content="https://i.imgur.com/Ou0PBfm.png">
<meta property="og:image" content="https://i.imgur.com/PW7ll1Q.png">
<meta property="og:image" content="https://i.imgur.com/huvFBco.png">
<meta property="og:image" content="https://i.imgur.com/BtcBwwU.png">
<meta property="og:image" content="https://i.imgur.com/7NxbFvF.jpg">
<meta property="og:image" content="http://chart.googleapis.com/chart?cht=tx&chl=\Large y = {ax + b \over 2}">
<meta property="og:image" content="https://i.imgur.com/U5riVAp.png">
<meta property="og:image" content="https://i.imgur.com/uZpZxdz.png">
<meta property="og:image" content="https://i.imgur.com/gdW7DwM.png">
<meta property="og:image" content="https://i.imgur.com/FF8SNDZ.png">
<meta property="og:image" content="https://i.imgur.com/ZrHPbGp.png">
<meta property="og:image" content="https://i.imgur.com/1GT46F0.png">
<meta property="og:image" content="https://i.imgur.com/hE5VgF2.png">
<meta property="og:image" content="https://i.imgur.com/qslxK9t.png">
<meta property="og:image" content="http://chart.googleapis.com/chart?cht=tx&chl=\Large Cost = {(predicted - real)^2}">
<meta property="og:image" content="https://i.imgur.com/jwReIP2.png">
<meta property="og:image" content="https://i.imgur.com/RGjpuX3.png">
<meta property="og:image" content="https://i.imgur.com/3UNJkcZ.png">
<meta property="og:image" content="https://i.imgur.com/4PmjIUv.png">
<meta property="og:image" content="https://i.imgur.com/BbwFVzg.png">
<meta property="og:image" content="http://chart.googleapis.com/chart?cht=tx&chl=\Large Error = {predicted - real}">
<meta property="og:updated_time" content="2017-08-01T09:11:37.582Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习（Machine Learning）简单学">
<meta name="twitter:description" content="本人对于机器学习这项技术具有强烈的兴趣，但是网络上的文献鱼龙混杂，很难找到真正适合入门级别的新手观看。前一阵子无意间在网络上看到了莫烦Python的教学视频，发现其中的内容丰富有趣并且具有很好的阶层学习框架。于是总结了一些精髓并加入了自己从事机器学习研究所工作的一些见解，总结了一些精华的部分以供大家快速入门和学习。 什么是机器学习机器学习（Machine Learning）是由一帮计算机科学家们希">
<meta name="twitter:image" content="https://i.imgur.com/PSM7e7e.png">
    
        <link rel="alternate" type="application/atom+xml" title="指尖の岁月" href="/atom.xml">
    
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="stylesheet" href="//unpkg.com/hexo-theme-material-indigo@latest/css/style.css">
    <script>window.lazyScripts=[]</script>
</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/new3.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/one2.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">Eternal</h5>
          <a href="mailto:617844662@qq.com" title="617844662@qq.com" class="mail">617844662@qq.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/EternalFeather" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://EternalFeather.github.io/AboutMe/" target="_blank" >
                <i class="icon icon-lg icon-link"></i>
                About Me
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">机器学习（Machine Learning）简单学</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">机器学习（Machine Learning）简单学</h1>
        <h5 class="subtitle">
            
                <time datetime="2017-07-24T05:48:31.000Z" itemprop="datePublished" class="page-time">
  2017-07-24
</time>


            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#什么是机器学习"><span class="post-toc-number">1.</span> <span class="post-toc-text">什么是机器学习</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#应用"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">应用</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#算法"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">算法</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#十大常见机器学习算法"><span class="post-toc-number">2.</span> <span class="post-toc-text">十大常见机器学习算法</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#线性回归（Linear-Regression）"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">线性回归（Linear Regression）</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#逻辑回归（Logistic-Regression）"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">逻辑回归（Logistic Regression）</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#决策树（Decision-Tree）"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">决策树（Decision Tree）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Gini"><span class="post-toc-number">2.3.1.</span> <span class="post-toc-text">Gini</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Information-Gain-amp-Entropy"><span class="post-toc-number">2.3.2.</span> <span class="post-toc-text">Information Gain & Entropy</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#支持向量机（Support-vector-machine-SVM）"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">支持向量机（Support vector machine,SVM）</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#朴素贝叶斯（Naive-Bayesian）"><span class="post-toc-number">2.5.</span> <span class="post-toc-text">朴素贝叶斯（Naive Bayesian）</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#K近邻（K-Nearest-Neighbors）"><span class="post-toc-number">2.6.</span> <span class="post-toc-text">K近邻（K Nearest Neighbors）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#欧式距离"><span class="post-toc-number">2.6.1.</span> <span class="post-toc-text">欧式距离</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#曼哈顿距离"><span class="post-toc-number">2.6.2.</span> <span class="post-toc-text">曼哈顿距离</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#明氏距离"><span class="post-toc-number">2.6.3.</span> <span class="post-toc-text">明氏距离</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#汉明距离"><span class="post-toc-number">2.6.4.</span> <span class="post-toc-text">汉明距离</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#K均值（K-means）"><span class="post-toc-number">2.7.</span> <span class="post-toc-text">K均值（K-means）</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#随机森林（Random-Forest）"><span class="post-toc-number">2.8.</span> <span class="post-toc-text">随机森林（Random Forest）</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#降维（Dimensionality-reduction）"><span class="post-toc-number">2.9.</span> <span class="post-toc-text">降维（Dimensionality reduction）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#主成分分析（PCA）"><span class="post-toc-number">2.9.1.</span> <span class="post-toc-text">主成分分析（PCA）</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#因子分析"><span class="post-toc-number">2.9.2.</span> <span class="post-toc-text">因子分析</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Gradient-Boost-amp-Adaboost"><span class="post-toc-number">2.10.</span> <span class="post-toc-text">Gradient Boost & Adaboost</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Adaboost"><span class="post-toc-number">2.10.1.</span> <span class="post-toc-text">Adaboost</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Gradient-Boost"><span class="post-toc-number">2.10.2.</span> <span class="post-toc-text">Gradient Boost</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#什么是神经网络（Neural-Network）"><span class="post-toc-number">3.</span> <span class="post-toc-text">什么是神经网络（Neural Network）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#神经网络的基本结构"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">神经网络的基本结构</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#神经元（Neuron）的激活函数（Activation-Function）"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">神经元（Neuron）的激活函数（Activation Function）</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#卷及神经网络（CNN）"><span class="post-toc-number">3.3.</span> <span class="post-toc-text">卷及神经网络（CNN）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#CNN常用结构"><span class="post-toc-number">3.3.1.</span> <span class="post-toc-text">CNN常用结构</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#递归神经网络（RNN）"><span class="post-toc-number">3.4.</span> <span class="post-toc-text">递归神经网络（RNN）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#RNN常用结构"><span class="post-toc-number">3.4.1.</span> <span class="post-toc-text">RNN常用结构</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#神经网络非监督式学习实现Autoencoder"><span class="post-toc-number">4.</span> <span class="post-toc-text">神经网络非监督式学习实现Autoencoder</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#生成对抗网络（GAN）"><span class="post-toc-number">5.</span> <span class="post-toc-text">生成对抗网络（GAN）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#GAN的应用"><span class="post-toc-number">5.1.</span> <span class="post-toc-text">GAN的应用</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#理解神经网络的“-黑盒子-”"><span class="post-toc-number">6.</span> <span class="post-toc-text">理解神经网络的“ 黑盒子 ”</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#如何优化神经网络（Optimization）"><span class="post-toc-number">7.</span> <span class="post-toc-text">如何优化神经网络（Optimization）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#神经网络梯度下降算法（Gradient-Descent）"><span class="post-toc-number">7.1.</span> <span class="post-toc-text">神经网络梯度下降算法（Gradient Descent）</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#如何评估神经网络的优越性"><span class="post-toc-number">8.</span> <span class="post-toc-text">如何评估神经网络的优越性</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#数据集评估"><span class="post-toc-number">8.1.</span> <span class="post-toc-text">数据集评估</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#为什么要对特征进行标准化（Normalization）"><span class="post-toc-number">9.</span> <span class="post-toc-text">为什么要对特征进行标准化（Normalization）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#如何标准化"><span class="post-toc-number">9.1.</span> <span class="post-toc-text">如何标准化</span></a></li></ol></li></ol>
        </nav>
    </aside>
    
<article id="post-Machine-Learning-Tutorial"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">机器学习（Machine Learning）简单学</h1>
        <div class="post-meta">
            <time class="post-time" title="2017-07-24 13:48:31" datetime="2017-07-24T05:48:31.000Z"  itemprop="datePublished">2017-07-24</time>

            


            

        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <p>本人对于机器学习这项技术具有强烈的兴趣，但是网络上的文献鱼龙混杂，很难找到真正适合入门级别的新手观看。前一阵子无意间在网络上看到了<a href="https://morvanzhou.github.io/tutorials/machine-learning/" target="_blank" rel="external">莫烦Python</a>的教学视频，发现其中的内容丰富有趣并且具有很好的阶层学习框架。于是总结了一些精髓并加入了自己从事机器学习研究所工作的一些见解，总结了一些精华的部分以供大家快速入门和学习。</p>
<h1 id="什么是机器学习"><a href="#什么是机器学习" class="headerlink" title="什么是机器学习"></a>什么是机器学习</h1><p>机器学习（Machine Learning）是由一帮计算机科学家们希望让计算机像人类一样思考而延伸出来的一门计算机理论。机器学习最早来自心理和生物科学，科学家们认为人和计算机其实没有什么差别，都是一大批相互连接的信息传递和存储元素所组成的系统。机器学习是一门典型的跨领域科学，其中包含了概率学、统计学等等方面。随着计算机性能的提升和计算机运算速度的升级，机器学习的应用才真正开始融入我们日常的生活当中。而不久的将来，机器学习必将成为人类探索机器世界的关键钥匙。</p>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>图像识别， AI对话式智慧型家居, 聊天机器人， 股市风险预测…</p>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>机器学习的实现方式多种多样，在程式语言中我们称之为算法。<br>Machine Learning的学习方式主要包括：</p>
<ul>
<li><strong>监督式学习（Supervised Learning）</strong><ul>
<li><strong>Input</strong>: Values and Labels</li>
<li><strong>Principle</strong>: 通过让计算机学习这些label来标记相应的value，从中找出它认为重要的部分作为判断依据。（<strong>既定规律</strong>）</li>
<li><strong>Example</strong>: Logistic Regression、Back Propogation Neural Network</li>
</ul>
</li>
<li><strong>非监督式学习（Un-Supervised Learning）</strong><ul>
<li><strong>Input: Values</strong></li>
<li><strong>Principle</strong>: 只提供value的情况下，计算机事先无法得知value所代表的含义以及需要学习的正确结果，这时候就需要让计算机自己学会分类不同的value，从而总结出不同value背后所隐藏的重要规律作为判断依据。（<strong>生成规律</strong>）</li>
<li><strong>Example</strong>: Apriori、K-Means</li>
</ul>
</li>
<li><strong>半监督式学习（Semi-Supervised Learning）</strong><ul>
<li><strong>Input</strong>: Values and A few Labels</li>
<li><strong>Principle</strong>: 这种学习方式主要让计算机考虑如何利用少量的label总结出最适合value的判断规则，从而引申到更大范围的value中。</li>
<li><strong>Example</strong>: Laplacisn SVM、Graph Inference</li>
</ul>
</li>
<li><strong>强化学习（Reinforcement Learning）</strong><ul>
<li><strong>Input</strong>: Environment and Set of Operations</li>
<li><strong>Principle</strong>: 通过将计算机设定在一个复杂的环境中，让机器去随机尝试各种可能的操作，并通过环境的回馈（正确加分，不正确扣分）的方式让机器的行为向加分的方面靠近，最终适应环境。</li>
<li><strong>Example</strong>: Alpha GO、Robot Control</li>
</ul>
</li>
</ul>
<p>Machine Leaning的算法主要分为这几类：</p>
<ul>
<li><strong>回归算法（Regression）</strong><ul>
<li>该算法主要是试图通过对误差的衡量来探索变量之间的关系问题。常见的回归算法包括：<strong>最小二乘法（Ordinary Least Square）</strong>、<strong>逻辑回归（Logistic Regression）</strong>、<strong>逐步回归（Stepwise Regression）</strong>、<strong>多元自适应回归样条（Multivariate Adaptive Regression Splines）</strong> 和 <strong>本地散点平滑估计（Locally Estimated Scatterplot Smoothing）</strong> 等。</li>
<li>常用的<strong>情形</strong>有：信用评估、度量成功率、预测收入水平、预测地震发生几率等等。</li>
</ul>
</li>
<li><strong>基于实例的算法（Instance-Based Algorithm）</strong><ul>
<li>该算法常常用来对决策性问题建模，通常会选取一批样本数据，然后根据某些特性和新数据样本的比较，通过匹配度来找到最佳的匹配相性。因此可以理解为 <strong>“赢家通吃”</strong> 的贪婪（Greedy）学习方式。</li>
<li>常见的算法包括：<strong>K-Nearest Neighbor（KNN）</strong>、<strong>学习矢量量化（Learning Vector Quantization，LVQ）</strong> 以及 <strong>自组织映射算法（Self-Organizing Map，SOM）</strong>。</li>
</ul>
</li>
<li><strong>正则化方式（Regular Expression）</strong><ul>
<li>该算法是基于回归算法的延伸，根据算法的复杂度对其进行的调整。正则化方法会对简单模型基于奖励而对复杂模型算法基于惩罚（一个类似强化学习的概念）。</li>
<li>常见的算法包括：<strong>Ridge Regression</strong>、<strong>Least Absolute Shrinkage and Selection Operator（LASSO）</strong> 和 <strong>弹性网络（Elastic Net）</strong>。</li>
</ul>
</li>
<li><strong>决策树（Decision Tree）</strong><ul>
<li>该算法根据数据的属性采用树状的结构建立决策模型，常常被用来解决<strong>分类</strong>和<strong>回归</strong>问题。</li>
<li>常见的算法包括：<strong>分类及回归树（Classification And Regression Tree，CART）</strong>、<strong>Iterative Dichotomiser 3（ID3）</strong>、<strong>随机森林（Random Forest）</strong> 以及 <strong>梯度推进（Gradient Boosting Machine，GBM）</strong>。</li>
</ul>
</li>
<li><strong>贝叶斯（Bayesian）</strong><ul>
<li>该算法是基于贝叶斯定理的一类演算法，主要也是来解决<strong>分类</strong>和<strong>回归</strong>的问题。</li>
<li>常见的算法包括: <strong>朴素贝叶斯（Naive Bayesian）</strong>、<strong>平均单依赖评估（Averaged One-Dependence Estimators，AODE）</strong> 以及 <strong>Bayesian Belief Network（BBN）</strong>。</li>
<li>常用范例：垃圾邮件分类、文章分类、情绪分类、人脸识别等。</li>
</ul>
</li>
<li><strong>基于核的算法（Kernel-Based Algorithm）</strong><ul>
<li>该算法最著名的应该是支持向量机（SVM）了，其将输入数据映射到一个高阶的向量空间中，在这些高阶空间里，有些分类或者回归问题就能得到解决。</li>
<li>常见的算法包括：<strong>支持向量机（Support Vector Machine，SVM）</strong>、<strong>径向基函数（Radial Basis Function，RBF）</strong> 和 <strong>线性判别分析（Linear Discriminate Analysis）</strong>。</li>
</ul>
</li>
<li><strong>聚类算法（Clustering）</strong><ul>
<li>该算法和回归类似，就是在处理分类问题的时候，通常以中心点或者分层的方式输入数据进行归并。所以聚类算法目的是找到数据的内部结构，以便按照最大的共同特征进行归类。</li>
<li>常见的聚类算法包括：<strong>K-Means算法</strong> 以及 <strong>期望最大化算法（Expectation Maximization，EM）</strong>。</li>
<li>聚类的关注特征也分为好多种，包括：质心、连通性、密度、概率、维度以及神经网络结构等。</li>
</ul>
</li>
<li><strong>关联法则（Association Rule）</strong><ul>
<li>该算法通过寻找最能解释数据变量之间关系的规则，从而找出大量多元数据集中的有用关联法则。</li>
<li>常见的算法包括：<strong>Apriori算法</strong> 和 <strong>Eclat算法</strong>。</li>
</ul>
</li>
<li><strong>遗传算法（Genetic Algorithm）</strong><ul>
<li>源自进化理论，淘汰弱者，适者生存。通过不断更新和淘汰的机制去选择最优的设计模型。后诞生的模型会继承先带模型的参数，并能够根据环境自我优化或消失。</li>
</ul>
</li>
<li><strong>人工神经网络（Neural Network）</strong><ul>
<li>该算法主要是模拟生物神经网络，属于模型匹配算法的一种。通常用于解决<strong>分类</strong> 和 <strong>回归</strong>的问题。人工神经网络是机器学习的一个庞大分支，有几百种不同的算法结构（包括深度学习）。</li>
<li>重要的神经网络算法包括：<strong>感知神经网络（Perceptron Neural Network）</strong>、<strong>反向传递（Back Propagation）</strong>、<strong>自组织映射（Self-Organizing Map，SOM）</strong>等。</li>
</ul>
</li>
<li><strong>深度学习（Deep Learning）</strong><ul>
<li>深度学习算法是基于人工神经网络的延伸，通过建立更复杂的神经网络结构来提升神经网络的效果。很多深度学习的算法是半监督式学习算法，用来处理少量未label的数据集。</li>
<li>常见的深度学习算法包括：<strong>受限波尔兹曼机（Restricted Boltzmann Machine，RBN）</strong>、<strong>Deep Belief Networks（DBN）</strong>、<strong>卷积网络（Convolutional Network）</strong> 和 <strong>堆栈式自动编码器（Stacked Auto-encoders）</strong>。</li>
</ul>
</li>
<li><strong>降低维度算法（Reduce Dimension）</strong><ul>
<li>与聚类相似，降低纬度算法也是试图分析数据内部的结构，不过该算法属于非监督学习的方式，在缺乏信息的情况下归纳或解释数据。这类算法利用高维度的数据作为监督的label使用，从而完成迁移的降维动作。</li>
<li>常见的算法包括：<strong>主成分分析（Principle Component Analysis，PCA）</strong>、<strong>偏最小二乘回归（Partial Least Square Regression，PLS）</strong> 和 <strong>投影追踪（Projection Pursuit）</strong>等。</li>
</ul>
</li>
</ul>
<h1 id="十大常见机器学习算法"><a href="#十大常见机器学习算法" class="headerlink" title="十大常见机器学习算法"></a>十大常见机器学习算法</h1><p>常用的机器学习算法，几乎可以用在所有的数据问题上：</p>
<h2 id="线性回归（Linear-Regression）"><a href="#线性回归（Linear-Regression）" class="headerlink" title="线性回归（Linear Regression）"></a>线性回归（Linear Regression）</h2><p>线性回归通常用于根据<strong>连续变量</strong>估计实际数值等问题上。通过拟合最佳的<strong>直线</strong>来建立<strong>自变量（X，features）</strong> 和 <strong>因变量（Y，labels）</strong> 的关系。这条直线也叫做回归线，并用<strong>Y = a* X + b</strong>来表示。</p>
<p>在这个等式中：</p>
<ul>
<li><code>Y</code> : 因变量（也就是Labels）</li>
<li><code>a</code> : 斜率（也就是Weights）</li>
<li><code>X</code> : 自变量（也就是Features）</li>
<li><code>b</code> : 截距（也就是Bias）</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/PSM7e7e.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>系数 <code>a</code> 和 <code>b</code> 可以通过<strong>最小二乘法</strong>（即让所有pairs带入线性表达式等号两边的方差和最小）获得。</p>
<h2 id="逻辑回归（Logistic-Regression）"><a href="#逻辑回归（Logistic-Regression）" class="headerlink" title="逻辑回归（Logistic Regression）"></a>逻辑回归（Logistic Regression）</h2><p>逻辑回归虽然名字中带有<strong>回归</strong>字样，但其实是一个<strong>分类</strong>算法而不是回归算法。该算法根据已知的一系列因变量估计<strong>离散的数值</strong>（0或1，代表假和真）。该算法通过将数据拟合进一个逻辑函数来预估一个事件发生的<strong>概率</strong>。由于其估计的对象是概率，所以输出的值大都在0和1之间。</p>
<p>逻辑回归通常用于解决二分类的问题，例如判断人是男是女等。逻辑回归就是通过人的一些基本性状特征来判断属于男女的概率。</p>
<p>从数学角度看，几率的对数使用的是<strong>预测变量的线性组合</strong>模型。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Probability of event occurence / not occurence</span></div><div class="line">odds = p / (<span class="number">1</span> - p)</div><div class="line">ln(odds) = ln(p / (<span class="number">1</span> - p))</div><div class="line">logit(p) = ln(p / (<span class="number">1</span> - p)) = b0 + b1X1 + b2X2 + ... + bnXn</div></pre></td></tr></table></figure></p>
<p>式子中 <code>p</code> 指的是特征出现的概率，它选用使观察样本可能性最大的值（<strong>极大似然估计</strong>）作为参数，而不是通过最小二乘法得到。</p>
<ul>
<li><p>那么为什么要取对数log呢？</p>
<ul>
<li>简而言之就是对数这种方式是复制阶梯函数最好的方法之一。</li>
</ul>
</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/hq1q9Z5.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>关于改进模型的方法：<ul>
<li>加入交互项（<strong>X1 * X2</strong>等）</li>
<li>对输入输出进行正规化</li>
<li>使用非线性模型</li>
</ul>
</li>
</ul>
<h2 id="决策树（Decision-Tree）"><a href="#决策树（Decision-Tree）" class="headerlink" title="决策树（Decision Tree）"></a>决策树（Decision Tree）</h2><p>该算法属于监督式学习的一部分，主要用来处理分类的问题，它能够适用于分类连续因变量。我们将主体分成两个或者更多的类群，根据重要的属性或者自变量来尽可能多地区分开来。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/8Nj3E0r.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>根据不同的决策属性，我们可以依次将输入进行分类，最终会得到一个标签（Label）。为了把总体分成不同组别，需要用到许多技术，比如<strong>Gini、Information Gain</strong> 和 <strong>Entropy</strong> 等。</li>
</ul>
<h3 id="Gini"><a href="#Gini" class="headerlink" title="Gini"></a>Gini</h3><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/ltVHIxt.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>图中的实际分配曲线（红线）和绝对平衡线（绿线）之间的<strong>面积</strong>为A，和绝对不平衡线（蓝线）之间的面积为B，则横纵坐标之间的比例的<strong>Gini系数</strong>为：</p>
<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=\Large {A \over A + B}" style="border:none;"></p>
<ul>
<li>A为零时，Gini系数为0，表示完全平衡。B为零时，Gini系数为1，表示完全不平衡。</li>
</ul>
<h3 id="Information-Gain-amp-Entropy"><a href="#Information-Gain-amp-Entropy" class="headerlink" title="Information Gain &amp; Entropy"></a>Information Gain &amp; Entropy</h3><p>在我们建立决策树的时候，常常会有许多属性，那么用哪一个属性作为数的根节点呢？这个时候就需要用到 <strong>信息增益（Information Gain）</strong> 来衡量一个属性区分以上数据样本的能力强弱。信息增益越大的属性作为数的根节点，就能使得这棵树更加简洁。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/9vwwsJt.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>以图中数据为例，要想知道信息增益，就必须先算出分类系的<strong>熵值（Entropy）</strong>。最终结果的label是yes或者no，所以统计数量之后共有9个yes和5个no。这时候<strong>P（“yes”） = 9 / 14，P（“no”） = 5 / 14</strong>。这里的熵值计算公式为：</li>
</ul>
<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=\Large Entropy(S) = {-(9 / 14) * log2(9 / 14) - (5 / 14) * log2(5 / 14)}" style="border:none;"></p>
<ul>
<li>之后就可以计算每一个属性特征的信息增益（Gain）了。以wind属性为例，Wind为Weak的共有8条，其中yes的有6条，no的有2条；为Strong的共有6条，其中yes的有3条，no的也有3条。因此相应的熵值为：</li>
</ul>
<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=\Large Entropy(Weak) = {-(6 / 8) * log2(6 / 8) - (2 / 8) * log2(2 / 8)}" style="border:none;"></p>
<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=\Large Entropy(Strong) = {-(3 / 6) * log2(3 / 6) - (3 / 6) * log2(3 / 6)}" style="border:none;"></p>
<ul>
<li>现在就可以计算Wind属性的<strong>信息增益</strong>了：</li>
</ul>
<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=\Large Gain(Wind) = {Entropy(S) -(8 / 14) * Entropy(Weak) - (6 / 14) * Entropy(Strong)}" style="border:none;"></p>
<h2 id="支持向量机（Support-vector-machine-SVM）"><a href="#支持向量机（Support-vector-machine-SVM）" class="headerlink" title="支持向量机（Support vector machine,SVM）"></a>支持向量机（Support vector machine,SVM）</h2><p>SVM是一种常用的机器学习分类方式。在这个算法过程中，我们将每一笔数据在<strong>N维度的空间中用点表示（N为特征总数，Features）</strong>，每个特征的值是一个坐标的值。</p>
<p>如果以二维空间为例，此时有两个特征变量，我们会在空间中画出这两个变量的分布情况，每个点都有两个坐标（分别为tuples所具有的特征值组合）。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/Ea3Jb95.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>现在我们找一条直线将两组不同的数据在维度空间中分开。分割的曲线满足让两个分组中的距离最近的两个点到直线的距离<strong>动态最优化</strong>（都尽可能最近）。</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/NGsSXtM.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>那么看到这里一定很多人和我一样有一个疑问，那就是这种线性分类的SVM和之前提到的逻辑回归（Logistic Regression）有什么<strong>区别</strong>呢？</li>
</ul>
<p>其实他们在二维空间的<strong>线性分类</strong>中都扮演了重要的角色，其主要区别大致可分为两类：</p>
<ul>
<li><p>寻找最优超平面的方式不同。</p>
<ul>
<li>形象来说就是Logistic模型找的超平面（二维中就是线）是尽可能让所有点都远离它。而SVM寻找的超平面，是只让最靠近的那些点远离，这些点也因此被称为<strong>支持向量样本</strong>，因此模型才叫<strong>支持向量机</strong>。</li>
</ul>
</li>
<li><p>SVM可以处理非线性的情况。</p>
<ul>
<li>比Logistic更强大的是，SVM还可以处理<strong>非线性</strong>的情况（经过优化之后的Logistic也可以，但是却更为复杂）。</li>
</ul>
</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/5seIoZJ.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<h2 id="朴素贝叶斯（Naive-Bayesian）"><a href="#朴素贝叶斯（Naive-Bayesian）" class="headerlink" title="朴素贝叶斯（Naive Bayesian）"></a>朴素贝叶斯（Naive Bayesian）</h2><p>在假设变量间<strong>相互独立</strong>的前提下，根据贝叶斯定理（Bayesian Theorem）可以推得朴素贝叶斯这个分类方法。通俗来说，一个朴素贝叶斯分类器假设分类的特性和其他特性不相关。朴素贝叶斯模型容易创建，而且在非监督式学习的大型数据样本集中非常有用，虽然简单，却能超越复杂的分类方法。其基本思想就是：对于给出的待分类项，求解<strong>在此项出现的条件下各个目标类别出现的概率</strong>，哪个最大，就认为此待分类项属于哪个类别。</p>
<p>贝叶斯定理提供了从P（c）、P（x）和P（x | c）计算后验概率P（c | x）的方法:</p>
<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=\Large P(c | x) = {P(x | c) P(c) \over P(x)}" style="border:none;"></p>
<p>式子中的变量表示如下：</p>
<ul>
<li>P（c | x）是已知预测变量（属性特征）的前提下，目标发生的后验概率。</li>
<li>P（c）是目标发生的先验概率。</li>
<li>P（x | c）是已知目标发生的前提下，预测变量发生的概率。</li>
<li>P（x）是预测变量的先验概率。</li>
</ul>
<p>举一个例子：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/gBuFCBd.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>这是一个训练资料集，提供一些身体特征，用来预测人的性别。此时假设特征之间独立且满足高斯分布，则得到下表：</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/eSwuOJV.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>通过计算方差、均值等参数，同时确认Label出现的频率来判断训练集的样本分布概率，P（male） = P（female） = 0.5。</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/qZPw7xC.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>此时给出测试资料，我们希望通过计算得到性别的后验概率从而判断样本的类型：</li>
</ul>
<p><strong>男子的后验概率</strong>:</p>
<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=\Large Posterior(male) = {P(male) P(height | male) P(weight | male) P(footsize | male) \over evidence}" style="border:none;"></p>
<p><strong>女子的后验概率</strong>:</p>
<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=\Large Posterior(female) = {P(female) P(height | female) P(weight | female) P(footsize | female) \over evidence}" style="border:none;"></p>
<p>证据因子（evidence）通常为常数，是用来对结果进行归一化的参数。</p>
<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=\Large Evidence = {(Posterior(female) + Posterior(male)) * evidence}" style="border:none;"></p>
<ul>
<li>因此我们可以计算出相应结果：</li>
</ul>
<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=\Large P(height | male) = {1 \over \sqrt{2\pi\sigma^2}}exp({-(6 - \mu^2) \over 2\sigma^2})" style="border:none;"></p>
<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=\Large P(weight | male) = ..." style="border:none;"></p>
<ul>
<li>最后可以得出后验概率:</li>
</ul>
<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=\Large Posterior Numerator(male) = {6.1984e^{-09}}" style="border:none;"></p>
<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=\Large Posterior Numerator(female) = {5.3778e^{-04}}" style="border:none;"></p>
<ul>
<li>因此女性的概率较大，我们估计结果为女性。</li>
</ul>
<h2 id="K近邻（K-Nearest-Neighbors）"><a href="#K近邻（K-Nearest-Neighbors）" class="headerlink" title="K近邻（K Nearest Neighbors）"></a>K近邻（K Nearest Neighbors）</h2><p>该算法可以用于分类和回归问题，然而我们更常将其被用于解决分类问题上。KNN能够存储所有的案例，通过对比周围K个样本中的大概率情况，从而决定新的对象应该分配在哪一个类别。新的样本会被分配到它的K个最近最普遍的类别中去，因此KNN算法也是一个基于距离函数的算法。</p>
<p>这些<strong>距离函数</strong>可以是欧氏距离、曼哈顿距离、明氏距离或是汉明距离。前三个距离函数用于<strong>连续函数</strong>，最后一个用于<strong>分类变量</strong>。如果K = 1，新的样本就会被直接分到距离最近的那个样本所属的类别中。因此选择K是一个关系到模型精确度的问题。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/7sGrxz0.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>如图所示，如果我们取K = 3，即为中间的圆圈内，我们可以直观地看出此时绿点应该被归为红三角的一类。而如果K = 5，此时延伸到虚线表示的圆，则此时绿点应该被归为蓝色的类。</li>
</ul>
<p>在选择KNN之前，我们需要考虑的事情有：</p>
<ul>
<li>KNN在K数量大的时候的计算成本很高。</li>
<li>变量（Features）应该先标准化（normalized），不然会被更高数量单位级别的范围带偏。</li>
<li>越是<strong>干净</strong>的资料效果越好，如果存在偏离度较高的杂讯噪声，那么在类别判断时就会收到干扰。</li>
</ul>
<h3 id="欧式距离"><a href="#欧式距离" class="headerlink" title="欧式距离"></a>欧式距离</h3><p>空间中点X = （X1，X2，X3，…，Xn）与点Y = （Y1，Y2，Y3，…，Yn）的欧氏距离为：</p>
<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=\Large d(x, y) := {\sqrt{(X1 - Y1)^2 + (X2 - Y2)^2 + ... + (Xn - Yn)^2}}" style="border:none;"></p>
<h3 id="曼哈顿距离"><a href="#曼哈顿距离" class="headerlink" title="曼哈顿距离"></a>曼哈顿距离</h3><p>在平面上，坐标（X1，X2，…，Xn）的点和坐标（Y1，Y2，…，Yn）的点之间的曼哈顿距离为:</p>
<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=\Large {|X1 - Y1| + |X2 - Y2| + ... + |Xn - Yn|}" style="border:none;"></p>
<h3 id="明氏距离"><a href="#明氏距离" class="headerlink" title="明氏距离"></a>明氏距离</h3><p>两点 P = (X1，X2，…，Xn) 和 Q = （Y1，Y2，…，Yn）之间的明氏距离为:</p>
<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=\Large {(|X1 - Y1|^p + |X2 - Y2|^p + ... + |Xn - Yn|^p)^{1 \over p}}" style="border:none;"></p>
<ul>
<li>其中p取1时为曼哈顿距离，p取2时为欧氏距离。</li>
</ul>
<h3 id="汉明距离"><a href="#汉明距离" class="headerlink" title="汉明距离"></a>汉明距离</h3><p>对于固定长度n，汉明距离是该长度字符串向量空间上的度量，即表示长度n中不同字符串的个数。</p>
<p>例子：</p>
<ul>
<li><strong>“toned”</strong> 和 <strong>“roses”</strong> 之间的汉明距离就是3。因为其中 <strong>t - &gt; r，n -&gt; s，d -&gt; s</strong> 三个字符不相同。</li>
</ul>
<h2 id="K均值（K-means）"><a href="#K均值（K-means）" class="headerlink" title="K均值（K-means）"></a>K均值（K-means）</h2><p>K-means方法是一种<strong>非监督式学习</strong>的算法，能够解决<strong>聚类</strong>问题。使用K-means算法将一个数据样本归入一定数量的集群中（假设有K个）中，每一个集群的数据点都是均匀齐次的，并且异于其它集群。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/WQlIGo4.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>K-means算法如何形成<strong>集群</strong>？</p>
<ul>
<li>给一个集群选择K个点，这些点称为质心。</li>
<li>给每一个数据点与距离最接近的质心形成一个集群，也就是K个集群。</li>
<li>根据现有的类别成员，找出每个类别的质心。</li>
<li>当有新的样本输入后，找到距离每个数据点最近的质心，并与质心对应的集群归为一类，计算新的质心位置，重复这个过程直到数据收敛，即质心位置不再改变。</li>
<li>如果新的数据点到多个质心的距离相同，则将这个数据点作为<strong>新的质心</strong>。</li>
</ul>
<p>如何决定K值？</p>
<ul>
<li>K-means算法涉及到集群问题，每个集群都有自己的质心。一个集群的内的质心和个数据点之间的距离的平方和形成了这个集群的平方值之和。我们能够直观地想象出当集群的内部的数据点增加时，K值会跟着下降（数据点越多，分散开来每个质心能够包揽的范围就变大了，这时候其他的集群就会被吞并或者分解）。<strong>集群元素数量的最优值</strong>也就是在集群的平方值之和最小的时候取得（每个点到质心的距离和最小，分类最精确）。</li>
</ul>
<h2 id="随机森林（Random-Forest）"><a href="#随机森林（Random-Forest）" class="headerlink" title="随机森林（Random Forest）"></a>随机森林（Random Forest）</h2><p>Random Forest是表示<strong>决策树总体</strong>的一个专有名词。在算法中我们有一系列的决策树（因此为<strong>森林</strong>）。为了根据一个新的对象特征将其分类，每一个决策树都有一个分类结果，称之为这个决策树<strong>投票</strong>给某一个分类群。这个森林选择获得其中（所有决策树）<strong>投票数最多</strong>的分类。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/xViexYM.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>Random Forest中的Decision Tree是如何形成的？</p>
<ul>
<li>如果训练集的样本数量为N，则从N个样本中用重置抽样的方式随机抽取样本。这个样本将作为决策树的训练资料。</li>
<li>假如有N个输入特征变量，则定义一个数字<strong>m &lt;&lt; M</strong>。m表示从M中随机选中的变量，这m个变量中最好的切分特征会被用来当成节点的决策特征（利用Information Gain等方式）。在构建其他决策树的时候，m的值<strong>保持不变</strong>。</li>
<li>尽可能大地建立每一个数的节点分支。</li>
</ul>
<h2 id="降维（Dimensionality-reduction）"><a href="#降维（Dimensionality-reduction）" class="headerlink" title="降维（Dimensionality reduction）"></a>降维（Dimensionality reduction）</h2><p>当今的社会中信息的捕捉量都是呈上升的趋势。各种研究信息数据都在尽可能地捕捉完善，生怕遗漏一些关键的特征值。对于这些数据中包含许多特征变量的数据而言，看似为我们的模型建立提供了充足的<strong>训练材料</strong>。但是这里却存在一个问题，那就是<strong>如何从上百甚至是上千种特征中区分出样本的类别呢？</strong>样本特征的<strong>重要程度</strong>又该如何评估呢？</p>
<ul>
<li>其实随着输入数据特征变量的增多，模型很难拟合众多样本变量（高维度）的数据分类规则。这样训练出来的模型不但<strong>效果差</strong>，而且<strong>消耗大量的时间</strong>。</li>
<li>这个时候，降维算法和别的一些算法（比如<strong>Decision Tree</strong>、<strong>Random Forest</strong>、<strong>主成分分析（PCA）</strong> 和 <strong>因子分析</strong>）就能帮助我们实现根据相关矩阵，压缩维度空间之后总结特征规律，最终再逐步还原到高维度空间的训练模式。</li>
</ul>
<h3 id="主成分分析（PCA）"><a href="#主成分分析（PCA）" class="headerlink" title="主成分分析（PCA）"></a>主成分分析（PCA）</h3><p>在多元统计分析中，PCA是一种分析、简化数据集的技术，经常用于减少数据集的维数，同时保留数据集中的<strong>对方差贡献最大</strong>的那些特征变量。</p>
<ul>
<li>该算法会根据不同维度的压缩（在这个维度上的<strong>投影</strong>）来测试<strong>各个维度对方差的影响</strong>，从而对每一个维度进行重新排序（影响最大的放在第一维度）。之后只需要取有限个数的维度进行训练，就能够保证模型拟合最佳的数据特征了。</li>
</ul>
<h3 id="因子分析"><a href="#因子分析" class="headerlink" title="因子分析"></a>因子分析</h3><p>该算法主要是从关联矩阵内部的依赖关系出发，把一些重要信息重叠，将错综复杂的变量归结为少数几个不相关的综合因子的多元统计方法。基本思想是：根据<strong>相关性大小</strong>把变量分租，使得同组内的变量之间相关性高，但不同组的变量不相关或者相关性低。每组变量代表一个基本结构，即公共因子。</p>
<h2 id="Gradient-Boost-amp-Adaboost"><a href="#Gradient-Boost-amp-Adaboost" class="headerlink" title="Gradient Boost &amp; Adaboost"></a>Gradient Boost &amp; Adaboost</h2><p>当我们想要处理很多数据来做一个具有高度预测能力的预测模型时，我们会用到Gradient Boost和AdaBoost这两种Boosting算法。<strong>Boosting算法</strong>是一种集成学习算法，它结合了建立在多个基础估计值上的预测结果，来增强单个估计值的准确度。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/eOKOw6J.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<h3 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h3><p>Bossting能够对一份数据建立多个模型（如分类模型），通常这些模型都比较简单，称为<strong>弱分类器（Weak Learner）</strong>。每次分类都将上一次分错的数据权重值调大（放大的圆圈），然后再次进行分类，最终得到更好的结果。最终所有学习器（在这里值分类器）共同组成完整的模型。</p>
<h3 id="Gradient-Boost"><a href="#Gradient-Boost" class="headerlink" title="Gradient Boost"></a>Gradient Boost</h3><p>与Adaboost不同的是，Gradient Boost在迭代的时候选择梯度下降的方向来保证最后的结果最好。损失函数（Loss function）用来描述模型的误差程度，如果模型没有Over fitting，那么loss的值越大则误差越高。如果我们的模型能够让损失函数值下降，说明它在不断改进，而最好的方式就是让函数在<strong>梯度的方向</strong>上改变。（类似神经网络的<strong>Gradient Descend</strong>）</p>
<h1 id="什么是神经网络（Neural-Network）"><a href="#什么是神经网络（Neural-Network）" class="headerlink" title="什么是神经网络（Neural Network）"></a>什么是神经网络（Neural Network）</h1><p>基于生物学的神经结构，将神经细胞的电信号传播机制应用到计算机结构中来，通过对信号传导和演变来组成网络架构。人工神经网络中的每一个“神经元”就是一个Neuron，用来以一定的算法改变输入的信号，从而改变传输的信息，达到对环境做出反应的目的。另一方面，通过神经网络产生的反应收到环境的反馈（做的好或不好），这些反馈和目标行为的误差会通过神经网络的反向传递从原先的路径传送回去，沿途中这些反馈信号会反过来刺激Neuron调整相应的参数从而使得下一次正向传递的结果能够更加贴近目标。如此往复便是整个神经网络训练的过程。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/umtL8L5.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>人类通过学习，能够掌握和判别事物的特征从而对事物的本质做出判断，而机器同样是利用这种机制建立起相应的“识别”模型，这些模型对不同的事物具有不同的反应强度，利用强度的不同来区别事物的本质。</p>
<h2 id="神经网络的基本结构"><a href="#神经网络的基本结构" class="headerlink" title="神经网络的基本结构"></a>神经网络的基本结构</h2><p>一个简单的神经网络由3个部分组成：</p>
<ul>
<li><strong>Input Layer</strong><ul>
<li>输入层，用来将资料喂给神经网络</li>
</ul>
</li>
<li><strong>Hidden Layer</strong><ul>
<li>隐藏层，用来尝试改变和调整神经网络的模型和数据的转化</li>
</ul>
</li>
<li><strong>Output Layer</strong><ul>
<li>输出层，用来将神经网络处理后的信号输出成最终的结果</li>
</ul>
</li>
</ul>
<h2 id="神经元（Neuron）的激活函数（Activation-Function）"><a href="#神经元（Neuron）的激活函数（Activation-Function）" class="headerlink" title="神经元（Neuron）的激活函数（Activation Function）"></a>神经元（Neuron）的激活函数（Activation Function）</h2><p>在神经网络学习的过程中，需要对输入的信号做出某种调整，才能真正得到最终的结果。<br>传统的激活函数包括：<br><strong>Sigmoid</strong>、<strong>TanHyperbolic(tanh)</strong>、<strong>ReLu</strong>、 <strong>softplus</strong>以及<strong>softmax</strong>函数</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/LT2BXvM.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>例如当我们输入一只猫，输入层神经网络会把信号传递给隐藏层的神经元。每一个接收到信息的神经元会通过自己现有的经验对信号做出判断，利用激活函数（activation function）来判断此时的神经元是否需要被激活。激活后的神经元就会对输入信号进行处理并传递给下一层的神经网络层，如此往复当信号传递到输出层时则会经由最终的刺激函数（一般为softmax）产生相应的结果确定输出的信号是属于哪一个标签（label）。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/vYgqqHJ.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>如果此时计算机得到了错误的结果，我们就会通过反向的传递将误差传导回去，改变<strong>所有</strong>的神经元参数，继而那些原本活跃的神经元就会被弱化，在下一次的神经传导过程中就会逐渐被激活函数淘汰。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/UHIK2YN.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>经过更新的神经网络能够在下一次迭代过程（epoch）中就会改变思路，转而尝试其他的判断方法。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/so90M0c.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>直到得到正确的结果，误差就会小到可以忽略，如此神经网络得以生成。</p>
<h2 id="卷及神经网络（CNN）"><a href="#卷及神经网络（CNN）" class="headerlink" title="卷及神经网络（CNN）"></a>卷及神经网络（CNN）</h2><p>卷积神经网络（Convolution Neural Network）在图片识别方面能够给出不错的结果。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/Y65bdvJ.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/TZ1jOeO.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>利用<strong>图片</strong>作为例子，任何输入的信号都会被转化成计算机能够识别的数字信号集合，例如矩阵（matrix）。<strong>文字</strong>也是一样的，我们把文字抽象成一个固定维度的向量，在这个维度空间中，每个字都是独立区别开来的，文字的多样性就有这些数字的排列组合来定义。这些信号集会通过输入层读取信息并进入神经网络中。<br>卷积神经网络就是其中的一种网络模式，我们可以把它分成<strong>卷积</strong>和<strong>神经网络</strong>两个部分来理解。</p>
<ul>
<li><strong>卷积</strong>：可以理解为对一个区域信号强弱的总体分析。通过卷积运算可以在一定的区域内总结有用信号的强弱分布，从而对一定区域内信号的变化情况能够有一个较好的认知。卷积能够增强信号的连续性，用区域单位代替点电位。</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/uOav6gV.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li><strong>神经网络</strong>：卷积神经网络利用批量过滤的方式，在大范围的信号中不断收集信息，每一次得到的区域信息都是区域中的一小块，之后从这些信息中总结出一些所谓的边缘信号（edges，例如：竖线，横线，斜线，圆圈等基本边缘，其可能分别代表人脸眼睛的左上角，中间，右上角等等部位的区域信息）。同样，用相同的方式从边缘信息组合的图像中总结出更大范围的边缘信息（例如：利用竖线，横线，圆圈等结构组合出整个眼睛）。最后将得到的结果传入全连接层的分类神经网络中就能得到相应的label了。</li>
</ul>
<p><strong>Example：</strong> </p>
<p><img src="https://i.imgur.com/fScPOCU.png" alt=""></p>
<p>图片的维度信息有长、宽和高，长和宽用来表示图片的信号集，高度则是表示颜色的信号分布。被白颜色只有1个高度单位，而彩色的图片则有R、G、B三种基本颜色的信息单位。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/VmMQcSI.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>利用批量过滤从图片中收集一定区域中的像素块，而输出的值就是一个高度更高，长和宽都更小的图片。这些图片存储的就是边缘（edges）信息。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/twaqQCR.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>反复进行同样的过滤步骤，就可以对图片的信息有更好的理解。之后再对结果进行分类就行了。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/ikChVno.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>在卷积的过程中，神经元可能无意中会丢失一些信息。池化（pooling）就是为了解决这样的问题而被设计出来的。既然我们的信息是在卷积过程中压缩的时候丢失的，那么我们就舍弃这个步骤，直接保留原本的长宽，最后在由池化层统一进行压缩长宽的动作。</p>
<h3 id="CNN常用结构"><a href="#CNN常用结构" class="headerlink" title="CNN常用结构"></a>CNN常用结构</h3><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/zugYmYR.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>比较流行的<strong>CNN结构</strong>先是输入信号，经过卷积层进行卷积运算，然后经过池化压缩长宽的维度。常用的是Max Pooling的结构：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/T3B3QjS.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>在区域中区最大值作为代表这个区域的信号。之后再次对结果进行相同的卷积和池化，进一步压缩信号。之后通过两个全连接层将信号传导给分类器进行分类预测。</p>
<h2 id="递归神经网络（RNN）"><a href="#递归神经网络（RNN）" class="headerlink" title="递归神经网络（RNN）"></a>递归神经网络（RNN）</h2><p>递归神经网络（Recurrent Neural Network）在自然语言处理和序列化信息分析方面能够给出不错的结果。如果说CNN是图像识别的代表性神经网络，那么RNN就是文字处理领域的“CNN”。</p>
<ul>
<li><strong>语言文字</strong>就是一个典型的<strong>序列化信号集</strong>，我们说出的每一句话之间，甚至每一个词之间都有先后关系的依赖，如果抛开字的先后顺序，我们的语言将会失去原本的含义。</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/afEYtWN.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>假设现在有许多不同的数据信号，如果神经网络只是基于当前的输入信号进行结果的预测，那么就相当于无视了所谓的连续规则，其中必然会丢失重要的时序信息。就好比做菜，酱料A要比酱料B先放，否则就会导致串味的现象。因此一般的NN结构无法让机器了解数据之间的关联。</p>
<p><strong>那么要如何做到让计算机也具有处理连续信号的能力呢？</strong></p>
<ul>
<li>从人的角度出发，不难想到的方式就是记住先前处理过的信号，并将这些信号一同作为输入传递到当前的神经网络中。</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/AUqu2Ss.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>我们将先前处理的结果存入记忆中，在分析当前信号时会产生新的记忆。由于记忆之间不会相互关联，因此我们可以直接将先前的记忆调用过来一起进行处理：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/D86UcOA.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>如此一来往复多次，神经网络就能携带长期的序列信号进行处理了。<strong>总结之前的流程：</strong></p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/MW1BRcu.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>在RNN运作过程中，每次的结果都会被存储为一个State状态信号，并通过不断迭代传递到下一个乃至更远的神经网络中去。</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/iQKtiQP.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>在RNN下一个时刻到来时，State状态同样会被存储成T+1时刻的State，但这是的 <strong>Y（t）</strong> 不再只是由 <strong>S（t+1）</strong> 来决定的，而是通过 <strong>S（t）</strong> 和 <strong>S（t+1）</strong> 共同处理 <strong>X(t+1)</strong> 得到的结果。因此这个State结构也可以用递回的方式来表示。</li>
</ul>
<h3 id="RNN常用结构"><a href="#RNN常用结构" class="headerlink" title="RNN常用结构"></a>RNN常用结构</h3><p>RNN的形式多种多样，一般需要根据处理的情况不同选择相应适合环境的模型进行建模。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/aZqWHqb.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>通常可以看到以下几种：</p>
<ul>
<li>如果是用于<strong>分类</strong>的话，例如在判断一句话的情感取向，判断是positive或者negative的情况下，倾向于使用根据最终结点的结果来输出判断的RNN：</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/Noc6AFL.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>如果是用于<strong>描述</strong>的话，例如通过一些集成度高的特征信号（图片等）来产生一个描述性的句子或者序列的情况下，倾向于使用根据单一输入来逐步读取时序信息的RNN：</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/2zP4G0h.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>如果是用于<strong>翻译</strong>的话，例如通过一段连续的输入信号来预测下一段连续输出信号的情况下，倾向于使用多对多输出的序列化RNN（Sequence-to-Sequence）：</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/LJSO3JG.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<h1 id="神经网络非监督式学习实现Autoencoder"><a href="#神经网络非监督式学习实现Autoencoder" class="headerlink" title="神经网络非监督式学习实现Autoencoder"></a>神经网络非监督式学习实现Autoencoder</h1><p>在神经网络训练过程中，往往会需要输入大量的信息，而这些信息对于计算机的学习来说具有十分巨大的负担。想想人类的学习过程，如果一次性塞给我们大量的信息，不但达不到很好的学习效果，还会浪费大量的时间。</p>
<p>因此我们需要一个特殊的神经网络来将原本的信息进行压缩，提取其中最具有代表性的信息，这个网络就是所谓的<strong>编码器（encoder）</strong>。之后再通过放大压缩后的信息，重现原始资料的全部信息，也就是 <strong>解码（decoder）</strong> 的过程。而我们所需要做的就是取得编码器压缩之后的简要信息，送入神经网络进行学习，从而达到我们的目的。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/SfOfLbo.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>压缩和解压的过程共同构成自编码（Autoencoder）的行为，通过训练编码器和解码器的神经网络结构，依据每次压缩前和解压后数据的对比情况来判断压缩的好坏程度，并利用反向传递来修正误差，从而最大程度上的压缩和还原原始信号。由于从头到尾我们所需要的输入信息为原始信号的信息，整个过程不需要对应的标签信息（label），因此autoencoder属于非监督学习的方式。</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/BidenDF.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>通常会使用到的部分是自编码的结果，也就是压缩过后的概括性讯息。我们建构的其他神经网络只需要对这些精髓的信息进行学习就行了。这样的方式不仅减少了神经网络的负担，还能达到很好的学习效果。</li>
</ul>
<p>自编码的思路和传统的主成分分析算法的精髓类似，都是试图从数据中抓住决定性的关键内容，来概括和分类数据的特征。相比于传统的降维算法中的PCA主成分分析方法，Autoencoder甚至能够取得更好的效果，因此也常被用来对原始数据进行<strong>降维</strong>。</p>
<h1 id="生成对抗网络（GAN）"><a href="#生成对抗网络（GAN）" class="headerlink" title="生成对抗网络（GAN）"></a>生成对抗网络（GAN）</h1><p>生成对抗网络（Generative Adversarial Net）不同于传统的FNN、CNN和RNN是将输入的数据和输出的结果通过某种关系联系起来的神经网络模型，GAN则是一种<strong>凭空生成结果</strong>的模型。</p>
<ul>
<li>当然所谓的 <strong>凭空</strong> 并不是真正意义上的<strong>无</strong>，而是通过一些随机的尝试（随机数组合）创造出一些东西。比如一张图片（像素集合）。</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/lOfWUK4.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>我们可以把这个随机尝试生成图片的网络比喻成一名新手画家，他们根据自己的灵感用现有的技术生成一些画作。一开始可能有了灵感但是由于作画技术的限制，往往无法生成理想中的图片。</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/tU6ZZEW.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>于是这名画家就找到了自己的好朋友新手鉴赏家，可是因为新手鉴赏家本身不具备良好的分辨能力，因此往往给出错误的回答。</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/a3YNuV0.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>这个时候就会有外部的干涉参与其中，通过一些标记好的资料来训练这名新手鉴赏家，让他一步步能够辨别画作的好坏。</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/Ou0PBfm.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>最重要的是在训练新手鉴赏家的过程中，随着鉴赏技术不断成熟，鉴赏家开始对新手画家的一些作品做出正确的判断和反馈。这时新手画家就会从这个新手鉴赏家手中得到<strong>真正的有用的标签（label）</strong>，进而利用这些标签改变自己的网络，让自己能够画得更好。</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/PW7ll1Q.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>总结之前的流程，就是新手鉴赏家这个神经网络利用从外部监督得到的反馈提升自己，然后再利用自己去训练另外一个神经网络，随着新手画家神经网络的不断提升，鉴赏家网络得知自己的能力已经无法鉴赏该画作时，就再次求助外部反馈。就在这一次一次地<strong>对抗</strong>中，两个神经网络就会越来越强大。</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/huvFBco.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>在GAN网络中，新手画家就是我们的<strong>生成器（Generator）</strong>，新手鉴赏家就是所谓的<strong>Discriminator（辨别器）</strong>，画家的每一幅画都是通过不同的数字排列组合成的像素矩阵，也就是我们说的图片。</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/BtcBwwU.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<h2 id="GAN的应用"><a href="#GAN的应用" class="headerlink" title="GAN的应用"></a>GAN的应用</h2><p>GAN因为能够通过随机组合产生新的数据，因而常被用在数据的合成和生成新数据的方面。</p>
<ul>
<li>其中一个重要的例子就是数据序列的加减法：</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/7NxbFvF.jpg" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>图中的二次元人物是通过GAN神经网络的学习，然后利用描述性的选项组合，来生成不同特征的人物图像的一个神经网络网络应用。</p>
<h1 id="理解神经网络的“-黑盒子-”"><a href="#理解神经网络的“-黑盒子-”" class="headerlink" title="理解神经网络的“ 黑盒子 ”"></a>理解神经网络的“ 黑盒子 ”</h1><p>神经网络的成功之处在于它能够从输入和输出的数据中总结出一个抽象的算法函式，基于这个函式的关系我们就能够对未知的数据进行预测。</p>
<p>例如：</p>
<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=\Large y = {ax + b \over 2}" style="border:none;"></p>
<p>这个就相当于一个已经训练好的神经网络模型，对于输入信号<code>X</code>通过网络的处理之后得到输出结果<code>Y</code>。</p>
<p>而神经网络建立的模型就像是把算法公式中所有参数进行一个<strong>封装</strong>，然后开放一个相应的<strong>接口(Interface)</strong>用于呼叫和取值。因此神经网络也被亲切地称之为 <strong>“ 黑匣子 ”</strong> 。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/U5riVAp.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>神经网络一般分为三个部分，输入和输出都是人类能够理解的信息，而中间的部分就是所谓的<strong>盲区</strong>。</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/uZpZxdz.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>如果我们将神经网络的中间层注意拆解后会发现输出的事物往往会是我们看不懂的东西，这就是为什么神经网络 <strong>“黑”</strong> 的原因了。对于人而言，我们在记忆复杂的环境和事物时往往会用一些自己熟知的<strong>记号来标记事物</strong>，使得我们能够更加清楚地记得事物的特征。计算机也是一样的：</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/gdW7DwM.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>我们知道神经网络处理的信息大都是数字集，通过神经层的分离可以看到这些数字集发生了改变，这些改变在人类看来无法理解，但事实上却是计算机利用自己的方式将这些事物通过它们捕捉到的特征信息转换成<strong>它们眼中的记号</strong>。也就是说计算机正在试图用自己能够理解的方式标记这些特征。</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/FF8SNDZ.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>在神经网络中，我们称人们能够识别的<strong>特征</strong>记作<strong>Features</strong>，而机器转换后的<strong>特征标记</strong>记作<strong>Feature Representation</strong>。</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/ZrHPbGp.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>利用手写数字的特征来理解的话，神经网络的Feature Representation就是空间中不同区域的分布状况。不同的位置聚集了不同的数字集合，落在不同的区域内就说明该输入属于哪一个输出。也就是说计算机把我们熟知的<strong>数字（也就是Features）</strong> 用 <strong>空间坐标区域（也就是Feature Representation）</strong> 来表示。</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/1GT46F0.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>理解神经网络的内部结构和Feature Representation的含义可以很好地利用 <strong>迁移学习（Transform Learning）</strong> 的方式来组合我们的神经网络，从而达到更好的效果。</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/hE5VgF2.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>例如我们已经训练好了从图片中解析物体的神经网络，它能够从图像的序列信息中提取关键的特征事物，此时只需要将输出层替换掉，再加入新的神经网络结构进行连接，就可以生成全新的模型。</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/qslxK9t.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>新的神经网络重新训练之后就能够具有全新的功能，利用原先的网络优势来拓展生成新的<strong>特征标记</strong>，一定程度上减少了神经网络训练的复杂度。基于先前的图像提取，能够从图中得到事物的<strong>特征信息（Feature Representation）</strong>，再利用新的网络将这些信息进一步转换成表示事物价格的<strong>特征信息</strong>，如此一来神经网络的功能就得以演化了。</li>
</ul>
<h1 id="如何优化神经网络（Optimization）"><a href="#如何优化神经网络（Optimization）" class="headerlink" title="如何优化神经网络（Optimization）"></a>如何优化神经网络（Optimization）</h1><p>优化（Optimization）一直是人类领先于其他生物而在环境中不断成长的重要因素，机器也不例外，通过优化的方式自我更新才能不被复杂的环境所淘汰。</p>
<h2 id="神经网络梯度下降算法（Gradient-Descent）"><a href="#神经网络梯度下降算法（Gradient-Descent）" class="headerlink" title="神经网络梯度下降算法（Gradient Descent）"></a>神经网络梯度下降算法（Gradient Descent）</h2><p>神经网络能够自我学习自我更新不仅仅归功于它能够学习并记忆输入和输出的规律，最重要的是它能够根据学习的规律进行自我调整以让自身适应这个变化的环境。那么机器学习模块又是如何进行优化的呢？答案就是所谓的<strong>梯度下降</strong>了。</p>
<ul>
<li>先前说过神经网络的自我调整是基于结果的反馈，也就是所谓的误差来修正自己：</li>
</ul>
<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=\Large Cost = {(predicted - real)^2}" style="border:none;"></p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/jwReIP2.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>Cost函式表达的结果近似可以看成一条平滑的二次曲线，而在更高纬度的层面上就是一个<strong>弯曲的面</strong>，越是接近曲面的底部，误差的Cost就会越小。而梯度下降（Gradient Descent）就是在这个曲面中通过微分的方式找到一个能够向最低点移动的方向，并以此作为动力开始优化自己。当达到最低点时，求导的结果和二次曲线相切，这个时候梯度就消失了，也就是所谓的最佳化状态。</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/RGjpuX3.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li><p>然而按照理论而言，这样的方式也太容易得到想要的结果了，那么神经网络的优化（Optimization）也太神了吧，其实这一切是<strong>很难实现</strong>的。</p>
</li>
<li><p>不同于之前所看到的梯度下降曲面，我们生活中的信号往往需要有许多的维度来表示，尤其是复杂的信号（例如图片或者文字）。这些信号在低纬度的时候几乎无法将他们区别分类，因此我们只能将他们丢到更高的维度上面进行非线性分割。这时候就会存在一个问题了，随着维度的提高，我们所熟知的曲面渐渐变得不再平滑了：</p>
</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/3UNJkcZ.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>这样的曲面反映出一个关键问题就是<strong>优化的不确定性</strong>。</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/4PmjIUv.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>在多维的复杂曲面中，我们能够找到不止一个梯度消失的点，而这些至低点并不都是我们所谓的<strong>最优解</strong>。当我们初始化的位置不同，我们的结果就会随着梯度下降（Gradient Descent）的优化模式寻找距离自己最近的一些至低点。如此一来<strong>不同的初始值</strong>就会很大程度上影响我们优化的结果。针对这个问题，目前比较好的解决方式就是给信号加上一个 <strong>动量（Momentum）</strong> 以至于在运动至最低点的时候，动量会趋势信号的Cost继续改变（此时梯度又恢复了）。如果我们设定的动量足以让信号摆脱当前的梯度曲面（说明曲面不够深，也就是所谓的局部最优解），信号就会继续去寻找一个更加<strong>难以摆脱的梯度曲面（更深）</strong>，如此一来就能够尽量靠近<strong>全局最优解</strong>。</li>
</ul>
<h1 id="如何评估神经网络的优越性"><a href="#如何评估神经网络的优越性" class="headerlink" title="如何评估神经网络的优越性"></a>如何评估神经网络的优越性</h1><p>机器学习的过程中，神经网络往往会存在一些问题，例如学习效率低，学习误差（loss）变化幅度摇摆不定，或是因为杂讯和信号太多没有办法找到有效的规律和结论。而这些问题可能来自<strong>数据</strong>、<strong>参数</strong>以及<strong>模型结构本身</strong>等各方面的因素。</p>
<h2 id="数据集评估"><a href="#数据集评估" class="headerlink" title="数据集评估"></a>数据集评估</h2><p>在评估数据和模型的吻合度上，我们需要对数据进行一个初步的认知，也就是确定数据集和结果之间的特征关系，也就是所谓的<strong>Features</strong>。这些Features能够很大程度地影响神经网络的学习效率。</p>
<ul>
<li>传统的机器学习算法通常会通过采用 <strong>Cross-Validation</strong> 的方式来对数据进行评估。也就是现将数据集依照6:2:2（不固定）的比例进行拆分，分别表示为<strong>训练集（Training Data）</strong>、<strong>验证集（Validating Data）</strong> 和 <strong>测试集（Testing Data）</strong> 三个部分。</li>
</ul>
<p>评估模型最终结果的好坏往往是测试集决定的，这里面会有训练的时候不曾出现过的输入信号，这也是对神经网络效能的一个<strong>考验</strong>。而要在学习的过程中让学习训练集的模型意识到不单单是要学好那些见过的部分，<strong>没见过的部分</strong>也需要充分地准备，这时候就会用到验证数据集的检验了。在训练完毕之后，我们重新划分3个资料集的比例和分布，就可以重新定义出新的训练资料了。在不断变换数据集的同时，我们可以对模型的<strong>参数进行更加科学的优化和分析</strong>。</p>
<ul>
<li>评价机器学习的方式（Evaluation Function）包括了<strong>误差（Error或Loss）</strong> 以及 <strong>精确度（Accuracy）</strong>，误差就是预测结果和实际结果的差值，而精确度就是在预测过程中的正确率了。</li>
</ul>
<p>有的时候在训练的时候往往结果让人满意，可是到了测试的时候结果却不尽人意，这又是为什么呢？</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.imgur.com/BbwFVzg.png" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>原来在训练过程中，神经网络太过优秀了，以至于它将自身优化成为了完全符合这个输入数据的一个模型。而一旦我们测试的输入和训练样本差别很大，就会让模型无从下手，这种现象就是所谓的过拟合（Overfitting）。</p>
<ul>
<li>比较常用来解决Overfitting的方式为<strong>Dropout</strong>，也就是在训练的过程中随机舍弃掉一些数据，从而让自己的模型留有一些变通的空间，来适应突发的情况。</li>
</ul>
<h1 id="为什么要对特征进行标准化（Normalization）"><a href="#为什么要对特征进行标准化（Normalization）" class="headerlink" title="为什么要对特征进行标准化（Normalization）"></a>为什么要对特征进行标准化（Normalization）</h1><p>现实中的数据可能来自不同的地方，不同来源的数据有各自的取值范围。而在学习的过程中，这些取值范围往往<strong>差距悬殊</strong>，这样就会对训练产生障碍。想象一下，如果我们两个权重矩阵M1和M2,我们给M1一个三位数量级的输入参数，给M2一个一位数量级的输入参数，会发生什么事情呢？答案很明显，当我们改变M1的参数时，对于总体的影响是十分巨大的，而相比之下想要达到这样的差距，就必须对M2进行很大幅度的调整。</p>
<h2 id="如何标准化"><a href="#如何标准化" class="headerlink" title="如何标准化"></a>如何标准化</h2><p>延续之前的例子，如果这时候的误差值是：</p>
<p><img src="http://chart.googleapis.com/chart?cht=tx&chl=\Large Error = {predicted - real}" style="border:none;"></p>
<p>那么对这个误差我们应该确保对所有的权重矩阵（Weight Matrix）具有类似的跨度。</p>
<p>通常用于标准化（Normalization）的方法有两种：</p>
<ul>
<li><p>一种是<strong>最小-最大标准化（Minmax Normalization）</strong>。它会将所有的数据按照一个缩放比例转换到0和1的区间中。对单独的特征而言，这个权重是唯一的（全局适用）。</p>
</li>
<li><p>另一种方法是<strong>标准正规化（Standard Normalization）</strong>。它会将所有数据转换成平均值为0，标准差（Std）为1的数据。</p>
</li>
</ul>
<p>这样的标准化问题不但能够平衡数据间的波动和差异，还能提高学习的效率，让机器学习能够正常地平衡每一个特征变数的优化和调节。</p>
<p>LICENCE： 图片摘录自网络引擎，未经授权请勿用于盈利性活动<br>更多详细内容 ： <a href="https://morvanzhou.github.io/" target="_blank" rel="external">Link</a></p>

        </div>

        <blockquote class="post-copyright">
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2017-08-01T09:11:37.582Z" itemprop="dateUpdated">2017-08-01 17:11:37</time>
</span><br>


        
        这里可以写作者留言，标签和 hexo 中所有变量及辅助函数等均可调用，示例：<a href="/2017/07/24/Machine-Learning-Tutorial/" target="_blank" rel="external">http://yoursite.com/2017/07/24/Machine-Learning-Tutorial/</a>
        
    </div>
    <footer>
        <a href="http://yoursite.com">
            <img src="/img/one2.jpg" alt="Eternal">
            Eternal
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Reinforcement-Learning/">Reinforcement Learning</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2017/07/24/Machine-Learning-Tutorial/&title=《机器学习（Machine Learning）简单学》 — 指尖の岁月&pic=http://yoursite.com/img/one2.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2017/07/24/Machine-Learning-Tutorial/&title=《机器学习（Machine Learning）简单学》 — 指尖の岁月&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2017/07/24/Machine-Learning-Tutorial/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《机器学习（Machine Learning）简单学》 — 指尖の岁月&url=http://yoursite.com/2017/07/24/Machine-Learning-Tutorial/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2017/07/24/Machine-Learning-Tutorial/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2017/07/25/Ubuntu-sogo/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">Ubuntu16.04如何安裝搜狗（Sogou）輸入法</h4>
      </a>
    </div>
  

  
</nav>



    


<section class="comments" id="comments">
    <div id="disqus_thread"></div>
    <script>
    var disqus_shortname = 'eternalfeather';
    lazyScripts.push('//' + disqus_shortname + '.disqus.com/embed.js')
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>













</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢您~~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.jpg" alt="打赏二维码">
        </div>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        

        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>博客内容遵循 <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span>
        </p>
    </div>
    <div class="bottom">
        <p><span>Eternal &copy; 2015 - 2017</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://yoursite.com/2017/07/24/Machine-Learning-Tutorial/&title=《机器学习（Machine Learning）简单学》 — 指尖の岁月&pic=http://yoursite.com/img/one2.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://yoursite.com/2017/07/24/Machine-Learning-Tutorial/&title=《机器学习（Machine Learning）简单学》 — 指尖の岁月&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://yoursite.com/2017/07/24/Machine-Learning-Tutorial/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《机器学习（Machine Learning）简单学》 — 指尖の岁月&url=http://yoursite.com/2017/07/24/Machine-Learning-Tutorial/&via=http://yoursite.com" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://yoursite.com/2017/07/24/Machine-Learning-Tutorial/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACM0lEQVR42u3a0W6DMAxGYd7/pdntJEZ0fptOxTm5qipo+Jjk2Y6PA6/z17p+f/f5usivXb9/bMmQIeO1jHO5yDXru9YP2tlXhgwZuzHuItg6mPKHW++43mv9bDJkyJCRXpPi+/vKkCFDBg+RwZalFyFDhgwZvBytFb39NPSxWlyGDBkvZPCu+/9//sj5hgwZMl7FqLX7+b2d1DB4KhkyZIxm8ADHxyZ46lYrem93kSFDxgYMPvhFAmhaBvPhjKDdJkOGjHGMGqmW/PEXxA8nZMiQMZVBWu2dgNs54OR7yZAhYzaDjziQpK0znIGSv/W/BBkyZIxmpAVkbVSidjCQPokMGTKmMjoJX9ror5XHKKzLkCFjM0Y6AJEOTxxg8VEPGTJkzGaQYa8Dr7T0TYfJgoEwGTJkDGLUWvOdAN0/yETDFjJkyBjH4KVjLUBzAL8X/U1kyJAxgpEG09oRZppu8utlyJCxD6MzMEGOHtOClofgx+Y+ZMiQ8cWM9OeeasPxBJQfSMiQIWMfRqcOJq/jqQGyYGZEhgwZgxjp+BcJjsXpD5IIypAhY0tG3OQqHTGmJWs8bCFDhoxxjDNcaQpIXkca0P+4V4YMGaMZacssLXfTUJ4OZMiQIWMfBg+yabDrHCTE6aMMGTI2YPDA12nlf/BKGTJkyAgPOIsZKD4ilSFDhow04Nba+rWy9pYkQ4aMDRi1oS7egCNDFbxklSFDxm6MfqLGm3H9kY50REOGDBkvZ/wAxEu8Ko/IMa4AAAAASUVORK5CYII=" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/main.min.js"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/search.min.js" async></script>








<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = 'Welcome back!';
            clearTimeout(titleTime);
        } else {
            document.title = '(つェ⊂) Hello!';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



</body>
</html>
